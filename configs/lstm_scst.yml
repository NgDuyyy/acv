#############################################
#  LSTM SCST Training Configuration
#  Finetuning with Self-Critical Sequence Training (CIDEr Optimization)
#############################################

############# MODEL CONFIG ##################

# Inherit arch from training (Must match previous config)
caption_model: updown
rnn_size: 1000
input_encoding_size: 1000
att_feat_size: 2048
fc_feat_size: 2048
att_hid_size: 512
rel_feat_size: 640

############# DATA PATH #####################

input_json: data/LSTM/data_label.json
language_eval_json: data/LSTM/val_reference.json
input_label_h5: data/LSTM/data_label.h5
input_fc_dir: data/features/features_extracted_fc
input_att_dir: data/features/features_extracted_att
input_box_dir: data/features/features_extracted_box
input_rel_dir: data/features/reltr_features.h5

# SCST Cache
# CiderScorer adds 'data/' prefix and '.p' suffix automatically
cached_tokens: LSTM/data-train-idxs

seq_per_img: 5
train_split: train
val_split: val

############# SCST SETTINGS #################

# Resume from best Cross-Entropy model
start_from: result/final_term/log_lstm_reltr

# Start SCST immediately (Epoch 0 of this run)
self_critical_after: 0

# Lower Learning Rate for Finetuning
learning_rate: 0.00005        # 5e-5
learning_rate_decay_start: -1 # Constant LR usually works best for SCST short runs

max_epochs: 30                # Usually converges faster than XE
patience: 5                   # Early stopping after 5 epochs of no improvement
batch_size: 32                # Keep same batch size

############# CHECKPOINT & LOGGING ##########

# New ID for this run
id: reltr_scst
checkpoint_path: result/log_lstm_reltr_scst
save_checkpoint_every: 3000
save_every_epoch: 1                # Save checkpoint and evaluate every epoch
language_eval: 1

############# INFERENCE #####################
beam_size: 1

#############################################
#  LSTM Training Configuration
#  Based on KTVIC DATASET - Bottom-Up + LSTM Decoder
#  YAML format compatible with opts.py parser
#  All keys stay on the top level (no TRAIN:/DATASET:/TEST: blocks)
#############################################

############# MODEL CONFIG ##################

caption_model: updown              # Bottom-Up & Top-Down Attention
rnn_size: 1000                     # Hidden size (standard for UpDown)
input_encoding_size: 1000          # Word embedding size
att_feat_size: 2048                # Feature size (ResNet101)
fc_feat_size: 2048                 # FC feature size
att_hid_size: 512                  # Attention MLP hidden size

############# DATA PATH #####################

# JSON describing captions (generated by prepro_labels.py)
input_json: data/LSTM/data_final.json
language_eval_json: data/LSTM/val_reference.json

# h5 file containing encoded captions
input_label_h5: data/LSTM/data_label.h5

# Feature directories
input_fc_dir: data/LSTM/features_fc
input_att_dir: data/LSTM/features_att
input_box_dir: data/LSTM/features_box

# Each image owns 5 captions
seq_per_img: 5

# Split names must match data.json
train_split: train
val_split: val

############# TRAINING SETUP ################

max_epochs: 30                     # Number of epochs
batch_size: 32                    # Batch size
patience: 10                       # Early stopping patience
learning_rate: 0.0005                # Learning rate
learning_rate_decay_start: 0       # Start decaying LR from epoch 0
scheduled_sampling_start: 0        # Start scheduled sampling from epoch 0
optim: adam                        # Optimizer choice
drop_prob_lm: 0.5                  # LSTM dropout
grad_clip_value: 0.1               # Gradient clipping
num_workers: 8                   # Number of workers for data loading
pin_memory: 0                      # Pin memory (set to 1 if using GPU)
val_images_use: -1                 # Number of validation images to use

# Optional but fixed defaults from opts.py
optim_alpha: 0.9
optim_beta: 0.999
optim_epsilon: 0.00000001
weight_decay: 0.0

############# CHECKPOINT & LOGGING ##########

save_checkpoint_every: 3000        # Save checkpoint every N iterations
save_every_epoch: 1                # Save checkpoint every epoch
checkpoint_path: result/log_lstm   # Directory to save checkpoints
language_eval: 1                   # Evaluate language metrics during training
language_eval_bleu_only: 1         # Evaluate all metrics (CIDEr, METEOR, etc.)

############# TEST / INFERENCE ##############

beam_size: 1                       # Beam size for inference
